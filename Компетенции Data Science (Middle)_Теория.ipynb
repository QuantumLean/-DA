{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5bf0d7",
   "metadata": {},
   "source": [
    "# Фонд оценочных средств (примеры экзаменационных билетов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb331d",
   "metadata": {},
   "source": [
    "# Уровень: Senior Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12149e56",
   "metadata": {},
   "source": [
    "# Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba51f0",
   "metadata": {},
   "source": [
    "1. Анастасия Никулина: Карьера в Data Science ТОП-50 вопросов на собеседовании https://www.youtube.com/watch?v=F1oPRLeNdmk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247471e",
   "metadata": {},
   "source": [
    "Область компктенций Аналитика данных сочетает в себе навыки сбора и обработки данных, анализ вариационных и временных рядов с целью поэтапного исследования информационной ценности данных. Базовый набор компетенций включает в себя:\n",
    "1. Извлечение и моделирование данных в конфигурации реляционных таблиц \n",
    "2. Обобщение, классификация и кластеризация данных, релевантных задачам исследований\n",
    "3. Проведения промежуточных математических вычислений, целью выявления логических закономерностей, зависимостей и взаимосвязей данных в формате значений и признаков; формирование гипотез\n",
    "4. Систематизация данных и моделирование архитектурных модулей данных для дальнейшего извлечения информации верхнего уровня\n",
    "5. Прогнозирование изменений информационного поля на основе вычисленных объективных закономерностей\n",
    "6. Анализ функции ошибок, валидация и верификация объектов данных с целью снижения погрешностей вычислений\n",
    "7. Проведение A/B/C анализа данных, поиск и моделирование релевантных функциональных зависимостей, интеграция формул в аналитическую архитектуру бизнес-решений; \n",
    "8. Взаимодействие и обогащение CI/CD процессов новыми алгоритмами оптимизации процессов коддинга; Подготовка обучающих датасетов; Адаптация витрин данных и интеграция в BI архитектуру    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75163a89",
   "metadata": {},
   "source": [
    "Для решения вышеперечисленных задач, Аналитик данных:\n",
    "1. Владеет теоретико-практическими инструментами для проведения математических итераций с данными (абстрактное моделирование, статистический анализ численных массивов данных, леинейная алгебра, теория вероятности (комбинаторика)\n",
    "2. Знает основы CI/CD и рабуту с удаленными репозиториями контроля версий кода; Культуру взаимодействия и коммуникации в командной работе; \n",
    "3. Умеет применять цифровые инструменты поиска, извлечения, хранения и обработки данных (математические вычисления); Умеет применять инструменты визуализации и адаптации данных для восприятия и анализа логическим и интуитивным способом"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857d594",
   "metadata": {},
   "source": [
    "Перечень основных программных инструментов Аналитика данных (middle)\n",
    "\n",
    "1. PostgreSQL / MySQL для извлечения и моделирования массивов данных в формате реляционных таблиц; не менее 10% команда, в том числе для моделирования вложенных запроссов (оконные функции); SQL-команды валидации и верификации данных (ORDER BY, JOIN, MERGE)\n",
    "2. Jupyter Notebook для агрегации процессов вычислений, объяснения и визуализации данных средствами объекто-ориентрированного языка программирования (Python, R); Выполнение вычислений при помощи специализированных библиотек формульных решений NumPy (моделирование и обработка массивов данных), SciPy (применение инженерных функций), Matplotlib / Seaborn (визуализация сложной логики), Pandas (работа с дата-фреймами и матрицами данных) \n",
    "3. Excel, Matlab для проведения соответствующих назначению операций над массивами данных (по необходимости)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a5c39",
   "metadata": {},
   "source": [
    "Для проверки уровня владения инструментами и знания операций преобразований данных в целевые объекты применяется следующий стек вопросов\n",
    "\n",
    "Младший аналитик данных / Junior Data Scentist\n",
    "1. SQL-запросы: Извлечение, обработка и моделирование реляционных таблиц данных; Индукционное моделирование таблиц данных (вложенные подзапросы, оконные функции, маркировка данных и применение ключевых идентификаторов; импорт и экспорт массивов в среду вычислений Python (SQLite, Pandas)  \n",
    "2. Математические методы статистического анализа: Прогнозирование (линейная и логистические регрессии, метод максимального правдоподобия, метод наименьших квадратов, нахождение коэффициента корреляции, выбор наилучшей модели, объясняющей вариативность выборки, построение экспоненциальной средней скользящей). Классификация (метод ближайшего соседа). Кластеризация (метод опорных векторов). Множественная регрессия (метод площадей треугольников). Сводка и группировка множеств (анализ вариационных рядов: среднее, медиана, квантили, квартили, дисперсия и плотность выборки).\n",
    "\n",
    "Аналитик данных / Middle Data Scientist\n",
    "3. Математические методы динамического (t>0) данных: Анализ временных рядов (адаптивные методы анализа: экспоненциальное скользящее среднее, модель трендов, мультипликативные модели с трендом и сезонностью, авторегрессия, следящий контрольный сигнал); навыки разработки feature selection (в т.ч. методом нахождения ассоциативных рядов)\n",
    "4. Python: Автоматизация и оптимизация вычислений с применением базовых функций языковой среды; Моделирование алгоритмов вычислений при помощи готовых функций; Применене готовых функций с использованием библиотек NumPy, Scipy, Pandas, Matplotlib и Seaborn. \n",
    "5. Kaggle, Git: ко-воркинг, навыки коммандного взаимодействия и развития универсальных компетенций (УК), общепрофессиональных компетенций (ОПК) и специальных компетенций Аналитика данных (ПК);\n",
    "\n",
    "Старший аналитик данных / Senior Data Scientist\n",
    "6. Стандарты хранения и обработки данных (ISO/ГОСТ), контроль доступа и обеспечение безопасности данных при работе с API-запросами;\n",
    "7. Адванта / Tableau / Power BI: Моделирование динамически обновляемых витрин и BI инструментов аналитики; обеспечение дискретных и непрерывных потоков данных для потребностей BI и ML; \n",
    "8. Feature selection: Поиск и выработка новых решений и в интеллектуальном анализе данных / data-mining; проведение научных исследований, верификация и валидация инструментов Data science  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84066a22",
   "metadata": {},
   "source": [
    "# Рекоммендуемые задания для проверки компетенций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fadbd2",
   "metadata": {},
   "source": [
    "# Владение SQL-компетенциями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7df9b6",
   "metadata": {},
   "source": [
    "1. Задание: из представленной таблицы данных необходимо вывести список высот горных массивов, которые выше среднего показателя от общего массива данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36e6d9",
   "metadata": {},
   "source": [
    "SELECT heighs *\n",
    "FROM heigh mounts, heigh mean\n",
    "WHERE heigh monts.id = heigh mean.id\n",
    "AND heigh monts > heigh mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5359f",
   "metadata": {},
   "source": [
    "2. SQL / GROUP BY (вложенные запросы). Вывести список Id горных систем с максимальной суммой высот "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39b5fb",
   "metadata": {},
   "source": [
    "WITH sum_heigh monts AS\n",
    "       (SELECT rocks_id, \n",
    "           SUM(heigh monts) heigh monts\n",
    "       FROM area\n",
    "       GROUP BY rocks_id)\n",
    "SELECT rocks_id\n",
    "FROM sum_heigh monts a\n",
    "WHERE a.heigh monts = (SELECT MAX(heigh monts\n",
    "                        FROM sum_heigh monts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437338a",
   "metadata": {},
   "source": [
    "3. SQL / MAX (с подзапросом). Вывести самую высокую вершину, которая не равна самой высокой верши в абсолютном значении всех величин  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0217d0",
   "metadata": {},
   "source": [
    "SELECT MAX(heigh monts)\n",
    "     AS Second_Highest_Heigh monts\n",
    "FROM area\n",
    "WHERE heigh monts !=\n",
    "                (SELECT MAX(heigh monts)\n",
    "                FROM area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a171c0",
   "metadata": {},
   "source": [
    "4. SQL / JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da6119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c8ab41",
   "metadata": {},
   "source": [
    "5. SQL / HAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e311b4",
   "metadata": {},
   "source": [
    "6. SQL / ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ffa03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "955ef359",
   "metadata": {},
   "source": [
    "7. Оконные функции: SQL / ROW_NUMBER (). Пронумеровать строки в таблице area  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0dc021",
   "metadata": {},
   "source": [
    "SELECT ROW_NUMBER () over(ORDER BY id)\n",
    "as id_num\n",
    "FROM area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee53b2b",
   "metadata": {},
   "source": [
    "8. Оконные функции: SQL / PARTITION BY. Пронумеровать строки в разрезе горных массивов rocks: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce017c9",
   "metadata": {},
   "source": [
    "SELECT ROW_NUMBER() over (PARTITION BY rocks ORDER BY heigh monts) heigh monts_sum\n",
    "FROM heigh monts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f8d52",
   "metadata": {},
   "source": [
    "# Статистика. Вариационные ряды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f06d8",
   "metadata": {},
   "source": [
    "ПАРАМЕТРЫ И МЕТРИКИ СТАТИСТИКИ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea59f20",
   "metadata": {},
   "source": [
    "Параметры и метрики статисткии. Среднее, среднее арифметическое\n",
    "\n",
    "Среднее - любое число для группы чисел x1, x2, x3 ... xn, находящееся между max и min\n",
    "\n",
    "Среднее арифметическое (AVG) - частный случай среднего. Сумма всех чисел вариационного ряда, поделенное на количество \n",
    "\n",
    "$$ AVG = \\sum_{i=1}^{n} (x_{1} + x_{2}... + x_{n}) / n$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272bac4",
   "metadata": {},
   "source": [
    "10. Параметры и метрики статистики. Матожидание\n",
    "\n",
    "Матожидание - разновидность среднего значения, означающее среднее значение случайной величины\n",
    "\n",
    "$$ EX = \\left\\{\\sum_{i}a_{i}p_{i};\n",
    "        \\int_{-\\infty}^{+\\infty} xf(x)dx\\right\\} $$\n",
    "        \n",
    "Например:\n",
    "\n",
    "$x_{i}$ = 1, 3, 4, 7$;\n",
    "\n",
    "$p_{i}$ = 0,1, 0,1, 0,2, 0,3$; \n",
    "\n",
    "$$EX = 1*0,1 + 3*0,1 + 4*0,2 + 7*0,3 = 3,3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85eda1b",
   "metadata": {},
   "source": [
    "11. Параметры и метрики статистики. Медиана.\n",
    "\n",
    "Медиана - это частный случай среднег арифметического: среднее арифметическое двух соседствующих чисел, стоящих в самой середине вариационного ряда. \n",
    "\n",
    "Нечетный ряд: \n",
    "5,8,2,4,1,7,5 -> 1,2,4,5,5,7,8\n",
    "\n",
    "$$M_{e} = \\frac{\\sum x_{n}}{n}$$\n",
    "$M_{e} = 5$\n",
    "\n",
    "Четный ряд:\n",
    "4,1,7,5,8,2 -> 1,2,(4,5),7,8,\n",
    "$$M_{e} = \\frac{\\sum x_{3}x_{4}}{n}$$\n",
    "$M_{e} = 4,5$\n",
    "\n",
    "\n",
    "Опрежделение медианы интервального ряда\n",
    "\n",
    "$$ M_{e} = x_{0} + h\\frac{0,5 * \\sum n - S_{me-1}}{n_{me}}$$\n",
    "\n",
    "$h$ - величина медианного интервала;\n",
    "$S_{me-1}$ - накопленная час ота интервала, предшествующая медианному;\n",
    "$n_{me}$ - частота медианного интервала"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1cd50",
   "metadata": {},
   "source": [
    "12. Параметры и метрики статистики. Квантили и Квартили\n",
    "\n",
    "Квантиль - величина, значение которой не превышает фиксированную вероятность. Квантиль 0,5 = число, ниже которой лежит половина выборки. Квантиль 0,75 = число, ниже которой лежит 3/4 выборки\n",
    "\n",
    "Квартиль, перцентиль, дециль = числа, соответственно равные 1/4, 1/100 и 1/10 интервалам рассматриваемой выборки "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc09bbe",
   "metadata": {},
   "source": [
    "13. Параметры и метрики статистики. Мода.\n",
    "\n",
    "Мода - это наиболее вероятное значение случайной величины вариационного ряда. \n",
    "\n",
    "$ mode X = argmax f(x) $; $X$ = непрерывная величина\n",
    "\n",
    "$ mode X = a_{argmax p_{i}} $; $X$ = дискретная величина\n",
    "\n",
    "Пример 1: 7,7,5,8,2,4,1; $ mode X = 7$\n",
    "\n",
    "Пример 2: 7,7,5,8,2,4,1,0,1; $ mode X = [1,7]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11934ea9",
   "metadata": {},
   "source": [
    "ПАРАМЕТРЫ РАЗБРОСА (ДИСПЕРСИЯ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1dbfcb",
   "metadata": {},
   "source": [
    "13. Параметры разброса. Дисперсия.\n",
    "\n",
    "Дисперсия = мера разброса значений случайной величины относительно ее математического ожидания:\n",
    "\n",
    "$$ DX = EX^2 - (EX)^2; DX = E(X - EX)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd52664",
   "metadata": {},
   "source": [
    "14. Параметры разброса. Среднеквадратичное отклонение\n",
    "\n",
    "Среднеквадратичное отклонение (MSE) = разница квадратуры отклонения случайной величины относительно ее мат.ожидания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6887bcf",
   "metadata": {},
   "source": [
    "15. Параметры разброса. Интерквантильный размах\n",
    "\n",
    "Интерквантильный размах (IQR) = расброс относительно квантилей вариационного ряда:\n",
    "\n",
    "$$ IQR = X_{0,75} - X_{0,25} $$\n",
    "\n",
    "В данном случае размах интерквантильного разброса определяется промежутком 0,75 и 0,25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c22ae",
   "metadata": {},
   "source": [
    "# Теория вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da36d9",
   "metadata": {},
   "source": [
    "16. Свойства вероятности. Возможные свойства отношений вероятностей:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655e9c2",
   "metadata": {},
   "source": [
    "- Вложенное событие \n",
    "$$ A (B) $$\n",
    "- Произведение событий \n",
    "$$ A * B $$\n",
    "- Пересечение событий \n",
    "$$ (A + B) $$\n",
    "- Дополение событий: B\\A (Происходит событие B, но не происходит событие A. Вероятность такого события задается выражением:\n",
    "$$ P(B \\ A) = P(B) - P(AB) $$\n",
    "Если A полностью находится в B, формула упрощается:\n",
    "$$ P(B \\ A) = P(B) - P(A) $$\n",
    "- Независимость событий: \n",
    "$$ P(AB) = P(A) * P(B) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93244772",
   "metadata": {},
   "source": [
    "17. Условная вероятность\n",
    "\n",
    "На экране семь полей. В одном из них скрыт приз. Вы выбираете одно из полей. После этого я открываю произвольным образом одну из полей. Вы видите, что там пусто. Вопрос: Измените ли Вы свое первоначальное решение? Объясните свое решениение? \n",
    "\n",
    "Ответ: нет, так как вероятность нахождения приза за выбранной панелью с   100 % / 7 = 14,2% возрастет до 100 % / 6 = 16%; \n",
    "\n",
    "Комментарий: Пусть событие A в предложенном примере - это попадание в призовое поле. Соответственно - попадание в любое иное место, кроме призового = событие B. Если известно, что событие B произошло, то вероятность того, что произойдет событие A при каждом последующем открытии возрастает: \n",
    "\n",
    "$$ P(A|B) = \\frac{P(AB)}{P(B)} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49832d1d",
   "metadata": {},
   "source": [
    "18. Полная вероятность: \n",
    "\n",
    "$$ P(A) = P(A|B)P(B) + P(A|\\bar{B})P(\\bar{B}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b1290",
   "metadata": {},
   "source": [
    "19. Формула Байеса\n",
    "\n",
    "Условные вероятности P двух событий А и B связаны друг с другом при помощи формулы Байеса:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5d741",
   "metadata": {},
   "source": [
    "20. Биноминальное распределение; Распределение Пуассона "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570abff",
   "metadata": {},
   "source": [
    "# Метрики качества. Регрессия\n",
    "\n",
    "Метрики качества определяют качество машинного обучения путем измерения отклонений от матожидания и(или) функции регрессии (линейной, логистической)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb997d",
   "metadata": {},
   "source": [
    "21.СРЕДНЕКВАДРАТИЧНАЯ ОШИБКА (MSE)\n",
    "\n",
    "- Легко оптимизируется в сочетании с методом градиентного спуска (ML);\n",
    "- Определяет связь дисперсии с функцией регрессии;\n",
    "- Удобен в вычислении и преобразованиях (в основе основание логарифма функция квадрата);\n",
    "- Достаточно чувствителен к аномалиям (по сравнению с вариацией по модулю);\n",
    "- Высокий коэффициент детерминации значения к признаку;\n",
    "- Широкий размах вариации даже при минимальном изменении признака; \n",
    "- Дисперсия ошибок создает оптимальные условия для настройки алгоритма на самообучение\n",
    "\n",
    "$$ MSE(a, X) = \\frac{1}{l}\\sum_{i=1}^{l}(a(x_{i}) - y_{i})^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff3a82",
   "metadata": {},
   "source": [
    "22.СРЕДНЯЯ АБСОЛЮТНАЯ ОШИБКА (MAE)\n",
    "\n",
    "- Сложнее минимизировать, т.к. у модуля производная не существует в нуле; \n",
    "- Менее чувствителен к аномалиям (по сравнению с MSE)\n",
    "- Устойчив к выбросам и хорошо подходит для \"грубой\" очистки данных\n",
    "- Низкий коэффициент детерминации значения к признаку;\n",
    "- Незначительный размах вариации даже при среднем изменении признака; \n",
    "- Дисперсия ошибки создает оптимальные условия для обучения в ручном режиме\n",
    "\n",
    "$$ MAE (a, X) = \\frac{1}{l}\\sum_{i=1}^{l}|a(x_{i}) - y_{i}| $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bdef1",
   "metadata": {},
   "source": [
    "# Метрики качества. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec01245",
   "metadata": {},
   "source": [
    "Метрики качества измеряются в двумерной матрице ошибок (confussion matrix), где \n",
    "$\\hat{y}$ = ответ алгоритма на объекте; \n",
    "$y$ = истинная метка класса на объекте\n",
    "\n",
    "$\\hat{y} = 1$ при $y = 1$ -> True Positive $(TP)$\n",
    "\n",
    "$\\hat{y} = 1$ при $y = 0$ -> False Positive $(FP)$\n",
    "\n",
    "$\\hat{y} = 0$ при $y = 1$ -> False Negative $(FN)$\n",
    "\n",
    "$\\hat{y} = 0$ при $y = 0$ -> True Negative $(TN)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246f2b0",
   "metadata": {},
   "source": [
    "24.ACCURACY (ACC) = % правильных ответов по отношению к общей массе ответов:\n",
    "\n",
    "$$ ACC = \\frac{\\sum_{i=n}^{p}}{\\sum_{i=1}^{n}} $$\n",
    "\n",
    "Хорошо применима , если классы хорошо сбалансированны по дисперсии и выбросам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60024af6",
   "metadata": {},
   "source": [
    "25.PRECISSION (PREC) = Точность\n",
    "\n",
    "$$ PREC(a, X) = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "Демонстрирует, насколько можно доверять классификатору при срабатывании."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873c443",
   "metadata": {},
   "source": [
    "26.RECALL (REC)= Полнота\n",
    "\n",
    "$$ REC(a, X) = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "Демонстрирует, на какой доле истинных объектов первого класса срабатывает алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59cf32",
   "metadata": {},
   "source": [
    "27.Арифметическое среднее точности и полноты:\n",
    "\n",
    "$$ A = \\frac{1}{2}(PREC + REC) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78213080",
   "metadata": {},
   "source": [
    "28.F-мера:\n",
    "\n",
    "$$ F = \\frac{2 * (PREC) * (REC)}{PREC + REC} $$ \n",
    "\n",
    "Усреднение происходит путем сглаживания минимальных значений Precission и Recall при помощи гармонического среднего, но с учетом неравномерной дисперсии и разбалансировки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694959e",
   "metadata": {},
   "source": [
    "# Продвинутый уровень компетенций "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774dc2b",
   "metadata": {},
   "source": [
    "29.$LogLoss$ = Логарифмическая функция потерь \n",
    "\n",
    "$\\hat{y_{i}}$ = ответ алгоритма на $i$-том объекте;\n",
    "\n",
    "$y_{i}$ = истинная метка класса на $i$-том объекте;\n",
    "\n",
    "$l$ = размер выборки:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7988a",
   "metadata": {},
   "source": [
    "$$ LogLoss = \\frac{1}{l}\\sum_{i=1}^{l} (y_{i}) * log(\\hat{y_{i}}) + (1 - y_{i}) * log(1 - \\hat{y_{i}})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf16a3",
   "metadata": {},
   "source": [
    "30.$Lift$ = Метрика прироста концентраций. \n",
    "\n",
    "Если необходимо измерить точность выборки по ее частию:\n",
    "\n",
    "$$ Lift = \\frac{PREC}{(TP + FN) / l} $$ \n",
    "\n",
    "... где $l$ = размер выборки\n",
    "\n",
    "Удобно применять для исследования подмножеств"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ef580",
   "metadata": {},
   "source": [
    "31.$ROC - AUC$ = ROC--кривая (Reciever Operationing Curve). \n",
    "\n",
    "Значение AUC -ROC имеет смысл вероятности того, что если были выбраны случайный положительный и случайный отрицательный объекты выборки $l$, ТО положительный объект будет > отрицптельного объекта по оценке принадлежности:\n",
    "\n",
    "По оси $X$:\n",
    "$ False Positive Rate $ = $(FPR)$; \n",
    "$$FPR = \\frac{FP}{FP + TN}$$ \n",
    "\n",
    "По оси $Y$:\n",
    "$True Positive Rate $ = $(TPR)$; \n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Чем ближе график стремитсся к точке (1, 1) тем выше качество алгоритма обучения; \n",
    "\n",
    "Чем больше выборка, тем более гладкий график"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4db807",
   "metadata": {},
   "source": [
    "31.$Gini$ = Коэффициент Джини (Gini). Данная метрика является частным случаем ROC-кривой: \n",
    "\n",
    "$$ Gini = 2AUC - 1 $$\n",
    "\n",
    "Используется при оценке предсказательных моделей в задачах бинарной классификаци в условиях сильной несбалансированности классов значения (целевой переменной)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b24141",
   "metadata": {},
   "source": [
    "# Алгоритмы классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f8522",
   "metadata": {},
   "source": [
    "33.В чем разница между задачами классификации и регрессии? \n",
    "\n",
    "Ответ: Регрессия относится к методам прогнозирования значения данных по признакам. Классификация относится к объяснению текущих значений по признакам и отнесения объекту к классу. \n",
    "\n",
    "Алгоритмы регрессии допустимо также применять для задач классификации: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353b3ce",
   "metadata": {},
   "source": [
    "34.Какие алгоритмы классификации и регрессии Вы знаете?\n",
    "\n",
    "Ответ: \n",
    "Алгоритмы регрессии: \n",
    "- Логистическая регрессия; \n",
    "- Линейная регрессия; \n",
    "- Деревья решений;\n",
    "- Случайные леса\n",
    "\n",
    "Алгоритмы классификации: \n",
    "- Логистическая регрессия; \n",
    "- k-ближайших соседей; \n",
    "- Support Vector Maschine (SVM) - метод опорных векторов;\n",
    "- Наивный Байес;\n",
    "- Деревья решений;\n",
    "- Случайные леса\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257dc40",
   "metadata": {},
   "source": [
    "35.Какие ЛИНЕЙНЫЕ и НЕЛИНЕЙНЫЕ алгоритмы Вам известны? В каких случаях каждая из классов алгоритмов применяется\n",
    "\n",
    "Ответ: \n",
    "ЛИНЕЙНЫЕ алгоритмы: \n",
    "- Линейная регрессия;  \n",
    "- Логистическая регрессия; \n",
    "- Линейный классификатор;\n",
    "- SVM Метод опорных векторов (с полиномиальными ядрами)\n",
    "\n",
    "Ответ:\n",
    "НЕЛИНЕЙНЫЕ алгоритмы:\n",
    "- Случайные леса\n",
    "- Xgboost\n",
    "- SVM Метод опорных векторов (с неполиномиальными ядрами)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee95b",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40acc6",
   "metadata": {},
   "source": [
    "36. Определение, свойства и функции логистической регрессии\n",
    "\n",
    "Логистическая регрессия $\\phi(z)$ - разновидность множественной линейной регрессии (МЛНК), общее назначение которой состоит в анализе взаимосвязей нескольких независимых переменных признаков $x_{i}$ с зависимой переменной значения $y_{i}$: \n",
    "\n",
    "$$ \\phi(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b12034",
   "metadata": {},
   "source": [
    "Независимые переменные (предиеторы или регрессоры) состоят из аргументов $x_{i}$ с весами $w_{i}$, сгруппированные в линейную комбинацию весов и признаков (аргументов) $z$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c15fe",
   "metadata": {},
   "source": [
    "$$ z = w^T x = w_{0}x_{0} + w_{1}x_{1} + ... + w_{n}x_{n} $$\n",
    "\n",
    "$$ z = \\sum_{i=0}^{n} w_{i}x_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1511c8",
   "metadata": {},
   "source": [
    "Функция $\\phi(z)$ сигмоид. Чем больше прииближается к 1, тем больше $z$ стремится к бесконечности\n",
    "\n",
    "Таким образом Логистическая регрессия помогает определить вероятность принадлежности искомого значения по множеству признаков $x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60dfaa",
   "metadata": {},
   "source": [
    "37. Подбор параметров логистической регрессии.\n",
    "\n",
    "Метод максимального правдоподобия: \n",
    "Максимизация функции правдоподобия эквивалентна максимизации ее логарифма\n",
    "\n",
    "Метод наименьших квадратов (МНК):\n",
    "основан на минимизации суммы квадратов отклонений $\\sum_{i}e_{i}^2$ отклонений от искомых значеий $y_{i}$: \n",
    "\n",
    "$$ \\sum_{i}e_{i}^2 = \\sum_{i}(y_{i} - f_{i}(x))^2 -> min (z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3383aa",
   "metadata": {},
   "source": [
    "# Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9afe30",
   "metadata": {},
   "source": [
    "38. Регуляризация - \n",
    "$ Регуляризация $ - это метод минимизации эффекта переобучения (OVERFITTING). \n",
    "\n",
    "Переобучение - это эффект, в результате которого нейронная сеть достаточно хорошо раблтает с учебной выборкой (см. метрики), но не определяет схожие поведенческие паттерны при работе с тестовой выборкой. \n",
    "\n",
    "Второй нежелательный эффект - мультиколлинеарность: выражается в фактической линейной зависимости предполагаемых независимых аргументов $x_{i}$\n",
    "\n",
    "Общий смысл регуляризации - \"прореживание\" путем зануления некоторых весов.\n",
    "\n",
    "Тип  регуляризации L2 - Ridge:\n",
    "\n",
    "$$ \\|w\\|^2 = \\sum_{j=1}^{d} w_{j}^2 $$\n",
    "\n",
    "Так. как он квадратичный и выпуклый, значит подойдет метод градиентного спуска. \n",
    "\n",
    "$$ Q(w,X) + \\lambda\\|w\\|^2 -> min(w) $$\n",
    "\n",
    "Параметр $\\lambda$ (лямбда) основной параметр регуляризации в методе Ridge и находится путем кросс-валидации результата обучения с эталонной выборкой \n",
    "\n",
    "Тип  регуляризации L1 - Lasso:\n",
    "\n",
    "$$ \\|w\\|_{1} = \\sum_{j=1}^{d}|w_{j}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7e18e",
   "metadata": {},
   "source": [
    "# Деревья решений. Теория графов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae267f0",
   "metadata": {},
   "source": [
    "Деревья решений основана на применении основного условного алгоритма \"Если... то...\". В простом варианте биноминарного распределения значений деревья решений представляют собой частный случай булевой логики, где значение \"ИСТИНА\" = 1, значение \"ЛОЖЬ\" = 0 и представляют собой упрощенную версию нейронной сети\n",
    "\n",
    "В сложных запросах деревья решения могут представлять несколько вариаций значений в зависимости от признаков. В большинстве объекто-ориентированных языков формируются как вложенные запросы.   \n",
    "\n",
    "Алгоритмы перехода сигнала от графа к графу отражают суть процесса обучения. При помощи множества повторяемых переходных итераций генерируются правила, определяющие выходной вектор значений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27798d8",
   "metadata": {},
   "source": [
    "$$ \\phi(z) = \\frac{1}{1 + n^{t}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709d0c8",
   "metadata": {},
   "source": [
    "... где $n$ = количество ложных значений (значение не удовлетворяет ни одному из признаков \"Если... то...\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200c442",
   "metadata": {},
   "source": [
    "Правила генерируются за счет обобщения множества отдельных наблюдений, которые описывают предметную область, являющейся частью предыдущей. Поэтому их называют индукционными правилами, а сам процесс обучения = индукцией деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e6d58",
   "metadata": {},
   "source": [
    "39. Способы построения деревьев решений: \n",
    "\n",
    "Вертикальная модели индукции (абсолютное равенство):\n",
    "$$ f(z) = \\frac{1}{1 + x^{j}}; z^J = a_{i} $$\n",
    "\n",
    "Способ развиения (отношение значения к порогу)\n",
    "$$ f(z) = \\frac{1}{1 + x^{j}}; z^J \\leq a_{i} $$\n",
    "\n",
    "Выбор значений $z_{i}^j$ будет произведен с учетом наименьшей вероятности ошибки $Q$: \n",
    "\n",
    "$$ Q(z_{i}^j, t) -> min (j,a) $$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830a71a",
   "metadata": {},
   "source": [
    "# Деревья решений. Классификация и регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2ce56",
   "metadata": {},
   "source": [
    "CART (Classification Trees) относится к методу распознавания (см. задачи классификации). Как и линейная регрессия в предикативных методах анализа, в классификации это не самый лучший, но основной исходный метод для построения более качественных классификаторов. Второе преимущество: интрпретируемость результатов (x, y) \n",
    "\n",
    "Термины и определения: \n",
    "\n",
    "Задачи классификации:\n",
    "- кластеризация (классификация)\n",
    "- регрессия\n",
    "- распознавание (классификация)\n",
    "\n",
    "Архитектура древа классификации\n",
    "- корневая выборка (корень)\n",
    "- родительский узел\n",
    "- потомок\n",
    "- конечный узел (листья)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1ce5a",
   "metadata": {},
   "source": [
    "40. Архитектура модели аналитической функции и ее элементы\n",
    "\n",
    "Cогласно тождеству Эйлера, функция, как модель множества чисел имеет признаки пяти базовых элементов. Универсальная модель аналитической формулы классификации:\n",
    "$$ f(x) = X^n_{n+1} + \\alpha * X_{n+1} + \\beta * X_{n+1} + \\epsilon^{X_{n}-t} $$ \n",
    "\n",
    "где:\n",
    "$X$ - значение переменной (основание натурального логорифма);\n",
    "$\\alpha$ - совокупность внутренних факторов, задаваемых машиной;\n",
    "$\\beta$ - все внешние факторы, корректируемые аналитиком (учителем); \n",
    "$\\epsilon$ - основание размерности ошибки; \n",
    "$t$ - доля ошибок от общей совокупности всех предыдущих итераций \n",
    "$n$ - порядок исследуемой переменной\n",
    "\n",
    "Внутренние параметры $\\alpha$ могут включать:\n",
    "\n",
    "$n_{j}$ - количество слоев древа;\n",
    "$j_{o}$ - количeство делений (расщеплений)\n",
    "$j$ - вероятность принадлежности к классу\n",
    "\n",
    "Внешние параметры $\\beta$ могут включать:\n",
    "\n",
    "$a_{i}$ - пороговое значение перехода на новый уровень классификации;\n",
    "$H_{n}$ - верхний предел чистоты {0, 1};\n",
    "$P_{i}$ - принадлежность к классу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b55507",
   "metadata": {},
   "source": [
    "41. Критерии информативности. Методы измерения уровня \"чистоты\" узлов древа\n",
    "\n",
    "РЕГРЕССИЯ\n",
    "\n",
    "Критерий информативности \"ДИСПЕРСИЯ\"\n",
    "\n",
    "$$ H_{Dispersion} = \\frac{1}{|X|} \\sum_{i\\in X} (y_{i} - \\bar{y}(X))^2 $$\n",
    "\n",
    "$$ \\bar{y} = \\frac{1}{|X|} \\sum_{i\\in X} y_{i} $$\n",
    "\n",
    "КЛАССИФИКАЦИЯ\n",
    "\n",
    "Критерий чистоты (информативности) \"ИДЕКС ДЖИННИ\" (Ginni Index)\n",
    "\n",
    "$$ H_{Ginni} = 1- \\sum_{j=1} p_{j}^2 $$ \n",
    "\n",
    "Критерий чистоты (информативности) \"ЭНТРОПИЯ\" (Enthropy): \n",
    "\n",
    "$$ H_{Enthropy} = - \\sum_{j=1} p_{j} * \\log_{2} p_{j} $$\n",
    "\n",
    "Критерий чистоты \"ОШИБКА КЛАССИФИКАЦИИ\" (Classification Error):\n",
    "\n",
    "$$ H_{Class Error} = 1 - \\max (p_{j}) $$\n",
    "\n",
    "Критерий относительной чистоты: (насколько выборка в узле стала чище после расщепления):\n",
    "\n",
    "$$ \\Delta H = H_{родителя} - (\\frac{n_{левый}}{n_{родителя}} * H_{левый} + \\frac{n_{правый}}{n_{родителя}} * H_{правый}) $$\n",
    "... где $H$ уровень однородности (чистоты) массива $P_{x}$, $n$ количество переменных в массиве узла. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b99f4d",
   "metadata": {},
   "source": [
    "42. Что такое переобучение.\n",
    "\n",
    "Переобучение - это формирование новых узлов потомков неограничиваемое количество раз (1 объект = 1 уровень)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe1451",
   "metadata": {},
   "source": [
    "# Алгоритмы деревьев решений: Градиентный бустинг (GBM) / XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f039f",
   "metadata": {},
   "source": [
    "43. Градиентный бустинг: функции, свойства, математическая модель.\n",
    "\n",
    "GBM является методологической разновидностью дерева решений и применяется в задачах Регрессии. Основной задача GBM = минимазация каждой последующей функции ошибки по отношению к предыдущей\n",
    "\n",
    "GBM является также разновидностью фрактальной модели. В геометрии аналог древу Пифагора. Итеративная модель состоит из двух шагов\n",
    "\n",
    "Первая итерация: \n",
    "\n",
    "$X$ - поле множества предикторов с $x_{i}$;\n",
    "\n",
    "$Y$ - множество прогнозируемых значений с $y_{i}$; \n",
    "\n",
    "$\\hat{Y}$ - полученное значение;\n",
    "\n",
    "$ \\epsilon_{i}= Y_{i} - \\hat{Y_{i}} $ - Ошибка первой итерации\n",
    "\n",
    "Вторая интерация:\n",
    "\n",
    "$ \\epsilon_{1} = Y_{1} $\n",
    "\n",
    "$ \\epsilon_{1} = Y; \\hat{\\epsilon}_{1} = \\hat{Y} $\n",
    "\n",
    "$ \\hat\\epsilon_{2} = \\epsilon_{1} - \\hat{\\epsilon}_{1} $ Ошибка второй итерации\n",
    "\n",
    "И так далее: разница двух соседствующих $\\epsilon $ становится значением  $Y$ для следующей итерации. В интерпретации Фридмена функция потерь $\\psi(y, f(x))$ это ни что иное как один из вариантов $\\epsilon_{1} - \\hat{\\epsilon}_{1} $:\n",
    "\n",
    "$$ f(x) = arg_{f(x)}min E_{y,x} \\psi(y, f(x)) $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bf429",
   "metadata": {},
   "source": [
    "Дифференцированный вариант функции, где $z_{i}$ аналог разницы фактического и ожидаемого значения:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d88f07",
   "metadata": {},
   "source": [
    "$$ z_{i} = - \\frac{d}{d f(x_{i})} \\ \\psi (y_{i}, f(x_{i}))|_{f(x_{i})=\\hat{f}(x_{i})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687760bc",
   "metadata": {},
   "source": [
    "Далее, если нормализировать функцию до универсальной модели, то мы обучаем регрессионную модель $g_{x}$, предикаты из ковариации $x_{i}$ заменим на $z_{i}$, а внутренние внутренние параметры на $\\rho$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53281c",
   "metadata": {},
   "source": [
    "...то $\\rho$ будет являться размером градиентного шага, который вычисляется по формуле:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d157b12",
   "metadata": {},
   "source": [
    "$$ \\rho = argmin_{\\rho} \\sum_{i=1}^{N} \\psi (y_{i}, \\hat{f}(x_{i} + \\rho g(x_{i})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243a875",
   "metadata": {},
   "source": [
    "Обновим все в формат функции и получим итоговую математическую модель итераций GBM:\n",
    "\n",
    "$$ \\hat{f}(x) <-  \\hat{f}(x) +\\rho g(x_{i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e849c42",
   "metadata": {},
   "source": [
    "44. Стек формул градиентного бустинга для линейной регрессии (Гаусс)\n",
    "\n",
    "Отклонение: $ \\frac{1}{\\sum \\psi_{i}} \\sum \\psi_{I}(y_{i} - f(x_{i}))^2 $\n",
    "\n",
    "Начальное значение: $ f(x) = \\frac{\\sum \\psi_{i} (y_{i} - o_{i})}{\\sum \\psi_{i}} $\n",
    "\n",
    "Градиент: $ z_{i} = y_{i} - f(x_{i}) $\n",
    "\n",
    "Значение узла древа: $ \\frac{\\sum \\psi_{i} (y_{i} - f(x_{i})}{\\sum \\psi_{i}} $\n",
    "\n",
    "Критерием остановки может быть заранее выбранное значение $ \\epsilon_{stop} = y_{i} - f(x_{i}) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42b0a6",
   "metadata": {},
   "source": [
    "# Алгоритмы Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8969f7",
   "metadata": {},
   "source": [
    "44. Что такое Xgboost и в чем его отличие от GBM?\n",
    "\n",
    "Xgboost является улучшенной модификацией GBM. Основные преимущества:\n",
    "\n",
    "- Параллельная обработка нескольких деревьев. \n",
    "Обучение происходит быстрее и без потери качества;\n",
    "\n",
    "- Высокая гибкость применения. \n",
    "Вариативность выбора целей и критериальных метрик для каждого пользователя;\n",
    "\n",
    "- Обработка пропущенных значений. \n",
    "Встроенная подпрограмма помогает минимизировать количество потерь необработанных аргументов и значений\n",
    "\n",
    "- Обрезка деревьев. \n",
    "Xgboost остановит процесс расщепления узла, если столкнется с отрицательным значением $\\epsilon_{i}$. В то же время, Xgboost сначала расщепляет на полную глубину, а затем, переходит к обрезке листьев. Положительное усиление в том, что такой обратный ход помогает в нейтрализации переобучения  \n",
    "\n",
    "- Встроенная кросс-валидация. \n",
    "В Xgboost можно делать кросс-валидацию с использованием релевантных значений весов параллельного процесса обучения. В результати качество чистоты узла можно существенно повысить за одну сессию "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec3333",
   "metadata": {},
   "source": [
    "45. В чем сходства и различия Градиентного бустинга и Деревьев решений\n",
    "\n",
    "Ответ: Деревья решений - это метод классификации (распознавания). Градиентный бустинг (GBM) - это один из алгоритмов метода. Существует три основных вида обучения моделей методом деревьев решений: \n",
    "\n",
    "- Бэггинг (Bagging). \n",
    "В этом случае часто рассматривают однородных слабых учеников, обучают их параллельно и независимо друг от друга, а затем объединяют их, следуя некоторому детерминированному процессу усреднения.\n",
    "\n",
    "- Бустинг (Boosting). \n",
    "В этом случае часто рассматривают однородных слабых учеников, обучают их последовательно адаптивным способом (слабый ученик зависит от предыдущих) и объединяет их, следуя детерминированной стратегии.\n",
    "\n",
    "- Стекинг (Stacking). \n",
    "В этом случае часто учитывают разнородных слабых учеников, изучают их параллельно и объединяют их, обучая метамодель для вывода прогноза, основанного на предсказаниях различных слабых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7686f",
   "metadata": {},
   "source": [
    "# Случайные леса (Random forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535c970",
   "metadata": {},
   "source": [
    "46. В чем преимущество применения пметода \"Случайные леса\"\n",
    "\n",
    "Отвтет: при помощи случайных лесов можно нормализовать качество обучения не прибегая к тестовой выборке и кросс-валидации, а также отображать наиболее важные признаки выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d347c",
   "metadata": {},
   "source": [
    "# Ошибки алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30581f52",
   "metadata": {},
   "source": [
    "47. Что может стать причиной ошибки алгоритмов?\n",
    "\n",
    "Ответ: основные факторы, которые могут повлиять на качество обучения - это :\n",
    "- Шум ()\n",
    "- Смещение ()\n",
    "- Разброс ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf2f46",
   "metadata": {},
   "source": [
    "48. Каковы основные методы борьбы с переобучением?\n",
    "\n",
    "Ответ: Переобучение выражается в способности обучаться только по указанной выборке без примененря способности обобщать и обучаться на других выборках. Алгоритм борьбы с переобуч5нием можно разделить на три этапа\n",
    "\n",
    "А). Выявление переобучения\n",
    "Б). Нормализация обучения\n",
    "В). Профилактика переобучения\n",
    "\n",
    "А). Методы выявления переобучения:\n",
    "\n",
    "А1. Тестовая (отложенная) выборка\n",
    "А2. Регуляризация\n",
    "А3. Кросс-валидация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42d39b",
   "metadata": {},
   "source": [
    "# Библиотеки машинного обучения и гипперпараметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5571333",
   "metadata": {},
   "source": [
    "49. Типы, функции и характеристики основных библиотек ML Python\n",
    "\n",
    "SkLearn\n",
    "TensorFlow\n",
    "Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ba449",
   "metadata": {},
   "source": [
    "50. Виды и характеристики гипперпараметров\n",
    "\n",
    "Регуляризация (управление мерой сложности)\n",
    "Степень полинома (в задачах с применением регрессионного анализа)\n",
    "Количество слоев (максимальная глубина дерева)\n",
    "Критерий чистоты (информативности) в переходе на следующий уровень  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
